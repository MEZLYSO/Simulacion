{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb18a7fc",
   "metadata": {},
   "source": [
    "# Regresion Logistica: Deteccion de Spam\n",
    "\n",
    "En este ejercicio se muestran los fundamentos de la regresion logistica planteando uno de los primeros problemas que fueron solucionados con el uso de tecnicas de ***Machine Learning***: Deteccion de SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554ac8e",
   "metadata": {},
   "source": [
    "***La regresion lineal ayuda a predecir eventos a futuro, mientras que la logistica nos ayuda a predecir una probabilidad.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36103544",
   "metadata": {},
   "source": [
    "### Enunciado del ejercicio\n",
    "Se propone la construccion de un sistema de aprendizaje automatico capaz de predecir si un correo determinado corresponde a un correo ***SPAM*** o no para esto se utilizara el siguiente DataSet: [DataSet](https://www.kaggle.com/datasets/imdeepmind/preprocessed-trec-2007-public-corpus-dataset)\n",
    "\n",
    "The corpus trec07p contains 75,419 messages:\n",
    "- 25,220 Ham\n",
    "- 50,199 SPAM\n",
    "\n",
    "These messages contitute all the messages delivered to a particular server between these dates \n",
    "\n",
    "- Sun, 8 Apr 2007 13:07:21 -0400\n",
    "- Fri, 6 Jul 2007 07:04:53 -0400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b32f2a",
   "metadata": {},
   "source": [
    "### 1.- Funciones complementarias\n",
    "\n",
    "En este caso practico relacionado con la deteccion de e-mails de SPAM, el DataSet del que se dispone esta formado por e-mails, con sus correspondientes cabeceras y campos adicionales. Por lo tanto requieren un preprocesamiento previa a que sean ingeridos por el algoritmo de ***Machine Learning***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cdcdb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Esta clase facilita el preprocesamiento de correos electronicos que poseen codigo HTML \n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self,d):\n",
    "        self.fed.append(d)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c320a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion se encarga de eliminar los tags HTML  que se encuentran en el texto del e-mail \n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "042c3820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phrack World News'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de eliminacion de los tags HTML de un texto\n",
    "t='<tr><td aling=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td></tr>'\n",
    "strip_tags(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ff9ab",
   "metadata": {},
   "source": [
    "Ademas de eliminar los posibles tags HTML que se encuentran en el correo electronico, deben de realizarse otras acciones de prepocesamiento para evitar que los mensjes contengan ruido innecesario. Entre ellas se encuentra la eliminacion de los signos de puntuacion, eliminacion de posibles campos de correo electronico que no son reelevantes o eliminacion de afijos de una palabra manteniendo unicamente la raiz de la misma (***Stemming***). La clase que se muestra a continuacion realiza estas transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56f445a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b34645fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.punctuation = list(string.punctuation)\n",
    "\n",
    "    def parse(self, email_path):\n",
    "        \"\"\"Parse an email.\"\"\"\n",
    "        with open(email_path, errors='ignore') as e:\n",
    "            msg = email.message_from_file(e)\n",
    "        return None if not msg else self.get_email_content(msg)\n",
    "    \n",
    "    def get_email_content(self, msg):\n",
    "        \"\"\"Extract the email content\"\"\"\n",
    "        subject = self.tokenize(msg['Subject']) if msg['Subject'] else[]\n",
    "        body = self.get_email_body(msg.get_payload(),\n",
    "                                   msg.get_content_type())\n",
    "        content_type = msg.get_content_type()\n",
    "        # Returning the content of the email\n",
    "        return {\"subject\": subject,\n",
    "                \"body\": body,\n",
    "                \"content_type\": content_type}\n",
    "    \n",
    "    def get_email_body(self, payload, content_type):\n",
    "        \"\"\"Extract the body of the email\"\"\"\n",
    "        body = []\n",
    "        if type(payload) is str and content_type == 'text/plain':\n",
    "            return self.tokenize(payload)\n",
    "        elif type(payload) is str and content_type == 'text/html':\n",
    "            return self.tokenize(strip_tags(payload))\n",
    "        elif type(payload) is list:\n",
    "            for p in payload:\n",
    "                body += self.get_email_body(p.get_payload(),\n",
    "                                            p.get_content_type())\n",
    "        return body\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Transform a text string in tokens. perform two main actions,\n",
    "        clean the punctuation symbols and do stemming of the text.\"\"\"\n",
    "        for c in self.punctuation:\n",
    "            text = text.replace(c, \"\")\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        tokens = list(filter(None, text.split(\" \")))\n",
    "        # Stemming of the tokens\n",
    "        return [self.stemmer.stem(w) for w in tokens if w not in self.stopwords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aab3f3",
   "metadata": {},
   "source": [
    "Lectura de un e-mail en formato Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ad37cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From RickyAmes@aol.com  Sun Apr  8 13:07:32 2007\n",
      "Return-Path: <RickyAmes@aol.com>\n",
      "Received: from 129.97.78.23 ([211.202.101.74])\n",
      "\tby speedy.uwaterloo.ca (8.12.8/8.12.5) with SMTP id l38H7G0I003017;\n",
      "\tSun, 8 Apr 2007 13:07:21 -0400\n",
      "Received: from 0.144.152.6 by 211.202.101.74; Sun, 08 Apr 2007 19:04:48 +0100\n",
      "Message-ID: <WYADCKPDFWWTWTXNFVUE@yahoo.com>\n",
      "From: \"Tomas Jacobs\" <RickyAmes@aol.com>\n",
      "Reply-To: \"Tomas Jacobs\" <RickyAmes@aol.com>\n",
      "To: the00@speedy.uwaterloo.ca\n",
      "Subject: Generic Cialis, branded quality@ \n",
      "Date: Sun, 08 Apr 2007 21:00:48 +0300\n",
      "X-Mailer: Microsoft Outlook Express 6.00.2600.0000\n",
      "MIME-Version: 1.0\n",
      "Content-Type: multipart/alternative;\n",
      "\tboundary=\"--8896484051606557286\"\n",
      "X-Priority: 3\n",
      "X-MSMail-Priority: Normal\n",
      "Status: RO\n",
      "Content-Length: 988\n",
      "Lines: 24\n",
      "\n",
      "----8896484051606557286\n",
      "Content-Type: text/html;\n",
      "Content-Transfer-Encoding: 7Bit\n",
      "\n",
      "<html>\n",
      "<body bgcolor=\"#ffffff\">\n",
      "<div style=\"border-color: #00FFFF; border-right-width: 0px; border-bottom-width: 0px; margin-bottom: 0px;\" align=\"center\">\n",
      "<table style=\"border: 1px; border-style: solid; border-color:#000000;\" cellpadding=\"5\" cellspacing=\"0\" bgcolor=\"#CCFFAA\">\n",
      "<tr>\n",
      "<td style=\"border: 0px; border-bottom: 1px; border-style: solid; border-color:#000000;\">\n",
      "<center>\n",
      "Do you feel the pressure to perform and not rising to the occasion??<br>\n",
      "</center>\n",
      "</td></tr><tr>\n",
      "<td bgcolor=#FFFF33 style=\"border: 0px; border-bottom: 1px; border-style: solid; border-color:#000000;\">\n",
      "<center>\n",
      "\n",
      "<b><a href='http://excoriationtuh.com/?lzmfnrdkleks'>Try <span>V</span><span>ia<span></span>gr<span>a</span>.....</a></b></center>\n",
      "</td></tr><td><center>your anxiety will be a thing of the past and you will<br>\n",
      "be back to your old self.\n",
      "</center></td></tr></table></div></body></html>\n",
      "\n",
      "\n",
      "----8896484051606557286--\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inmail = open('./datasets/trec07p/data/inmail.1').read()\n",
    "print(inmail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb5304",
   "metadata": {},
   "source": [
    "Parsear el e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4d1de18",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/maury/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/share/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Simulacion/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Simulacion/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/maury/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/share/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m p = \u001b[43mParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m p.parse(\u001b[33m\"\u001b[39m\u001b[33mdatasets/trec07p/data/inmail.1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mParser.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.stemmer = nltk.PorterStemmer()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28mself\u001b[39m.stopwords = \u001b[38;5;28mset\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.punctuation = \u001b[38;5;28mlist\u001b[39m(string.punctuation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Simulacion/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Simulacion/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Simulacion/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Simulacion/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/maury/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/share/nltk_data'\n    - '/home/maury/Sources/Simulacion/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "p = Parser()\n",
    "p.parse(\"datasets/trec07p/data/inmail.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e7c6",
   "metadata": {},
   "source": [
    "### Lectura del indice\n",
    "\n",
    "Estas funciones complementarias se encargan de cargar en memoria la ruta de cada correo electronico y su etiqueta correspondiente {ham, spam}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01df462",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=open(\"datasets/trec07p/full/index\").readlines()\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "DATASET_PATH=\"datasets/trec07p\"\n",
    "\n",
    "def parse_index(path_to_index, n_elements):\n",
    "    ret_indexes = []\n",
    "    index = open(path_to_index).readlines()\n",
    "    for i in range(n_elements):\n",
    "        mail = index[i].split(\" ../\")\n",
    "        label = mail[0]\n",
    "        path = mail[1][:-1]\n",
    "        ret_indexes.append({\"label\": label, \"email_path\":os.path.join(DATASET_PATH, path)})\n",
    "    return ret_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(index):\n",
    "    p=Parser()\n",
    "    pmail=p.parse(index[\"email_path\"])\n",
    "    return pmail,index[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = parse_index(\"datasets/trec07p/full/index\", 10)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cb4f8",
   "metadata": {},
   "source": [
    "## 2.- Preprocesamiento de los datos del DataSet\n",
    "\n",
    "Con las funciones presentadas anteriormente se permite la lectura de los correos electronicos de manera programatica y el preprocesamiento de los mismos para eliminar aquellos componentes que no resultan de utilidad para la deteccion de correos SPAM. Sin embargo cada uno de los correos sigue estando representado por un diccionario de Python con una serie de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca416e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el indice y las etiquetas en memoria\n",
    "index=parse_index('datasets/trec07p/full/index',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c35122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el primer correo \n",
    "import os \n",
    "\n",
    "open(index[0]['email_path']).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser el primer correo \n",
    "mail, label = parse_email(index[0])\n",
    "print(\"Label:\", label)\n",
    "print(mail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9450f67",
   "metadata": {},
   "source": [
    "El algoritmo de regresion logistica no es capaz de ingerir text como parte del dataset.  Por lo tanto deben aplicarse, una serie de funciones adicionales que transforme el texto de los correos parseados en una representacion numerica.\n",
    "\n",
    "Aplicacion de CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984500f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Preparacion del e-mail en una cadena de texto\n",
    "prep_email = [\" \".join(mail['subject']) + \" \".join(mail['body'])]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit(prep_email)\n",
    "print(\"e-mail:\",prep_email, \"\\n\")\n",
    "print(\"Caracteristicas de entrada:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c86f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform(prep_email)\n",
    "print(\"\\nValues:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de la matriz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "prep_email = [[w] for w in mail['subject'] + mail['body']]\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "x = enc.fit_transform(prep_email)\n",
    "\n",
    "print(\"Features: \\n\",enc.get_feature_names_out())\n",
    "print(\"\\n Values: \\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4efd03",
   "metadata": {},
   "source": [
    "Funciones auxiliares para el preprocesamiento del DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prep_dataset(index_path,n_elements):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    indexes=parse_index(index_path,n_elements)\n",
    "    for i in range(n_elements):\n",
    "        print(\"\\n Parsing e-mail: {0}\".format(i+1),end='')\n",
    "        mail, label = parse_email(indexes[i])\n",
    "        X.append(\" \".join(mail['subject']) + ' '.join(mail['body']))\n",
    "        Y.append(label)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35c1f2",
   "metadata": {},
   "source": [
    "## 3.-Entrenamiento del Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca10aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer unicamente un subconjunto de 100 correos\n",
    "\n",
    "X_train, Y_train = create_prep_dataset('./datasets/trec07p/full/index',100)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aeab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.toarray())\n",
    "print('\\nFeatures: ',len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57969ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.DataFrame(X_train.toarray(),columns=[vectorizer.get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cd017",
   "metadata": {},
   "source": [
    "Entrenamiento del algoritmo de regresion logistica con el DataSet preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdace58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e18625",
   "metadata": {},
   "source": [
    "### 4.- Prediccion \n",
    "Lectura de un DataSet de corres electronicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4623af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer 150 correos electronicos de nuestro DataSet y nos quedamos con los 50 ultimos, esto 50 correos electronicos no se han utilizado \n",
    "# para entrenar el algoritmo\n",
    "X,Y=create_prep_dataset('./datasets/trec07p/full/index',150)\n",
    "X_test = X[100:]\n",
    "Y_test = Y[100:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b9a77",
   "metadata": {},
   "source": [
    "Preprocesamiento de los correos electronicos con el vectorizador creado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3beb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf4c29",
   "metadata": {},
   "source": [
    "Prediccion del tipo de correo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediccion: \\n\",Y_pred)\n",
    "print(\"\\nEtiquetas reales: \\n\",Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9d87a",
   "metadata": {},
   "source": [
    "Evaluacion de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb957808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: {:3f}'.format(accuracy_score(Y_test,Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e8f93",
   "metadata": {},
   "source": [
    "### 5.- Aumentando el DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer 12,000 correos electronicos para entrenar el algoritmo y 2,000 para realizar pruebas\n",
    "X,Y=create_prep_dataset(\"./datasets/trec07p/full/index\",12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d777d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = X[:10000],Y[:10000]\n",
    "X_test,Y_test = X[10000:],Y[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ffe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a494a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9400efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(Y_test,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
